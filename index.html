<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>LAD: LoRA-Adapted Denoiser</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <style>
        body { font-family: 'Inter', sans-serif; max-width: 800px; margin: auto; line-height: 1.6; padding: 2rem; background-color: #ffffff; color: #333; }
        h1, h2, h3 { color: #222; font-weight: 600; }
        p { margin-bottom: 1rem; }
        a.button { display: inline-block; padding: 10px 18px; margin: 0.5rem 0.5rem 0.5rem 0; background-color: #f5f5f5; color: #333; border-radius: 6px; border: 1px solid #ccc; text-decoration: none; transition: background-color 0.3s; }
        a.button:hover { background-color: #e0e0e0; }
        iframe { border: 1px solid #ccc; border-radius: 8px; width: 100%; height: 500px; }
        table { border-collapse: collapse; width: 100%; margin-top: 1rem; }
        th, td { border: 1px solid #ccc; padding: 0.5rem; text-align: left; }
        code, pre { background: #f0f0f0; padding: 0.5em; border-radius: 4px; display: block; overflow-x: auto; }
        figure { text-align: center; margin: 2rem 0; }
        figcaption { font-size: 0.9rem; color: #666; margin-top: 0.5rem; }
    </style>
</head>
<body>

    <h1>LAD: LoRA-Adapted Denoiser</h1>

    <div style="text-align: center;">    
      <p><strong>Authors:</strong> Ruurd Kuiper<sup>1</sup>, Maarten van Smeden<sup>1</sup>, Lars de Groot<sup>1</sup>, Ayoub Bagheri<sup>2</sup></p>
      <p><sup>1</sup>UMC Utrecht &nbsp;&nbsp; <sup>2</sup>Utrecht University</p>
    
      <a href="https://huggingface.co/spaces/Ruurd/tini-lad" target="_blank" class="button" style="background-color: #3B82F6; color: white;">Huggingface</a>
      <a href="https://huggingface.co/spaces/Ruurd/tini" target="_blank" class="button" style="background-color: #22C55E; color: white;">Extensive Demo</a>
      <a href="https://medium.com/ai-advances/think-before-you-speak-5611bcbbbd4c" target="_blank" class="button" style="background-color: #6B7280; color: white;">Read the Blogpost</a>
    </div>
    
    <h2>Demonstration</h2>
    <p>Interact with the LAD model below to see its generative capabilities in action. The demo is running the <strong>LAD 8B Instruct</strong> model, which has been fine-tuned to follow user instructions.</p>
    <iframe src="https://ruurd-tini-lad.hf.space"></iframe>

    <h3>Core Innovations (TL;DR)</h3>
    <ul>
        <li><b>Efficient Adaptation:</b> Adapting for diffusion with <strong>only several hours of training on a single GPU.</strong></li>
        <li><b>Rapid Inference:</b> Often requires <strong>less steps than number of output tokens</strong> for simpler queries.
        <li><b>Scalable test time compute:</b>Answer quality of more difficult questions can be improved by increasing iterations.</li>
        <li><b>Structural Denoising:</b> Uses masking and structural noise for <strong>full-sequence refinement</strong> in each step.</li>
        <li><b>Noiseless Refinement:</b> Enables iterative improvement of complete sequences <strong>without remasking.</strong></li>
    </ul>
        
    <h2>Motivation and Approach</h2>
    <p>Autoregressive models, while powerful, are constrained by a strict left-to-right generation process that limits efficiency and bidirectional reasoning. Diffusion-based models offer a promising, flexible alternative but have faced challenges in adapting to discrete text data. LAD (LoRA-Adapted Denoiser) was created to bridge this gap, offering a framework for non-autoregressive generation that is both flexible and efficient.</p>
    <p>We introduce a novel approach that adapts pretrained LLaMA models for iterative, bidirectional sequence refinement. By repurposing these autoregressive models, LAD benefits from their extensive world knowledge while breaking free from their sequential decoding limitations. This is achieved without costly full-model retraining, making it a practical solution for developing advanced generative capabilities.</p>

    <h2>How LAD Works: Core Innovations</h2>
    <p>The core of LAD is its combined structural and masked training objective which enables flexible inference. We modify the standard 
        causal attention mechanism of the frozen LLaMA 3.2 backbone to be fully bidirectional, allowing the model to consider the entire 
        context of the sequence at once. On top of this frozen base, we train lightweight Low-Rank Adaptation (LoRA) adapters. This 
        parameter-efficient approach preserves the original model's knowledge while drastically reducing the computational resources 
        required for adaptation.</p>

    <h2>Training and Efficiency</h2> 
    <p>The LAD 8B Instruct model was trained with an emphasis on extreme efficiency. We ran only 100,000 training iterations 
        using a context length of 256 and a batch size of 8—amounting to just 200 million training tokens. For comparison, Meta’s 
        LLaMA 3 8B was trained on 15 trillion tokens, and the LlaDa diffusion model on 2.3 trillion, meaning LAD used <strong>just 0.001% 
        and 0.008%</strong> of their respective token counts.
    </p> 
    <p>Training was completed in approximately <strong>10 hours on a single NVIDIA A100 GPU.</strong> By leveraging a frozen backbone with LoRA 
        adapters, LAD significantly lowers the barrier to entry for training powerful, non-autoregressive text models, avoiding 
        the high computational costs of training from scratch or full-parameter finetuning.
    </p>
    
    <h2>Visualizing the Denoising Process</h2>
    
    <div style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
      <figure style="flex: 1; min-width: 300px; max-width: 48%;">
        <img src="figure_lad_remask.png" alt="With Remasking" style="width: 100%;">
        <figcaption><strong>Left:</strong> Denoising with <em>remasking</em> after each step. Tokens are re-masked with decreasing frequency to allow iterative correction. The model replaces the masking tokens while also improving the remaining text.</figcaption>
      </figure>
    
      <figure style="flex: 1; min-width: 300px; max-width: 48%;">
        <img src="figure_lad_noremask.png" alt="Without Remasking" style="width: 100%;">
        <figcaption><strong>Right:</strong> Denoising <em>without remasking</em>. After initializing with mask tokens, the model improves the text directly without even without remasking tokens. This generally reduces flexibility but speeds up inference.</figcaption>
      </figure>
    <p><em>Note:</em> In both cases, the number of iterations is shorter than the number of tokens, in contrast to autoregressive tokens where the number of iterations is equal to the number of output tokens.</p>
    </div>

    
    <h2>Preliminary Results</h2>
    
    <table>
      <tr><th>Benchmark</th><th>Score</th></tr>
      <tr><td>ARC-Easy</td><td>88.5</td></tr>
      <tr><td>ARC-Challenge</td><td>81.0</td></tr>
      <tr><td>MMLU</td><td>60.5</td></tr>
      <tr><td>HellaSwag</td><td>70.0</td></tr>
    </table>
    
    <p><em>Note:</em> Scores were computed on a random 200-example subset per dataset. Full benchmark results will follow.</p>


    <h2>Citation</h2>
    <p>A full paper is forthcoming and will be submitted for peer review. In the meantime, if you use LAD in your work, please cite this preliminary version:</p>
    <pre><code>@misc{kuiper2025lad,
      author       = {Ruurd Kuiper and Maarten van Smeden and Lars de Groot and Ayoub Bagheri},
      title        = {LAD: LoRA-Adapted Denoiser},
      year         = {2025},
      howpublished = {\url{https://ruurdkuiper.github.io/tini-lad/}},
      note         = {Work in progress}
}</code></pre>

    <p>We welcome feedback and collaboration as we continue to develop and evaluate LAD.</p>

</body>
</html>
