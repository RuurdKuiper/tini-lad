<!DOCTYPE html>
<html lang="en">
<head>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@600&display=swap" rel="stylesheet">
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>LAD: LLaMA Adapted Denoiser</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>

  <h1>LAD: LLaMA Adapted Denoiser</h1>

  <div style="text-align: center; margin-bottom: 2rem;">
  <p><strong>Ruurd Kuiper</strong><sup>1</sup>, <strong>Maarten van Smeden</strong><sup>1</sup>, <strong>Lars de Groot</strong><sup>1</sup>, <strong>Ayoub Bagheri</strong><sup>2</sup></p>
  <p><sup>1</sup>University Medical Center Utrecht ‚Äî <sup>2</sup>Utrecht University</p>
  </div>
  <div style="text-align: center; margin: 1.5rem 0;">
  <a href="https://huggingface.co/spaces/Ruurd/tini" target="_blank" style="padding: 10px 18px; margin: 0 10px; background-color: #f5f5f5; color: #333; border-radius: 6px; border: 1px solid #ccc; text-decoration: none;">Try on Hugging Face</a>
  </div>

  <p><strong>Note:</strong> Paper coming out soon. If you're interested in discussing the model, feel free to reach out.</p>

  <p>
    This is an interactive demo of a <strong>diffusion-style language model</strong> built on LLaMA 3.1 8B and fine-tuned with LoRA adapters.
    Unlike conventional LLMs that generate text token-by-token, text diffusion models iteratively denoise corrupted sequences to form coherent output.
    And unlike conventional diffusion models, this implementation enables refinement <strong>without intermediate renoising!</strong>
  </p>

  <h2>Live Demo</h2>
  <iframe
    src="https://ruurd-tini-lad.hf.space"
    frameborder="0"
    width="100%"
    height="400"
    style="border: 1px solid #ccc; border-radius: 8px;"
  ></iframe>

  <h2>Features:</h2>
  <h4>Unique to this model</h4>
  <ul>
    <li><strong>Noiseless convergence:</strong> For simple questions, the model can converge without intermediate noising.</li>
    <li><strong>Reduced inference cost:</strong> Many queries resolve in fewer steps than the output token length.</li>
    <li><strong>Etremely efficient training:</strong> Finetuned via LoRA in a few hours on a single GPU.</li>
  </ul>

  <h4>Inherent diffusion features:</h4>
  <ul>
    <li><strong>Scalable test-time compute:</strong> More iterations improve response quality.</li>
    <li><strong>Reverse reasoning:</strong> Supports non-sequential updates, allowing reasoning from right to left.</li>
    <li><strong>Error correction:</strong> Initial erroneous tokens can be corrected.</li>
  </ul>

  <h2>üîß Settings Overview</h2>
  <ul>
    <li><strong>Disable Intermediate Noising:</strong> Speeds up convergence for short, factual queries.</li>
    <li><strong>Pause Between Steps:</strong> Slows the denoising process for visual clarity.</li>
    <li><strong>Iterations:</strong> Controls how many refinement steps are performed.</li>
  </ul>

  <h2>üñçÔ∏è Visualization Legend</h2>
  <ul>
    <li><span style="color:red;">Red tokens</span>: Masked (noised) tokens to be regenerated.</li>
    <li><span style="color:green;">Green tokens</span>: Newly generated tokens since last step.</li>
  </ul>

  <h2>üß™ Example Prompts</h2>
  <p><strong>Noiseless mode (short, factual):</strong><br/>
    <code>What's the capital of France?</code></p>

  <p><strong>With intermediate noising (longer context):</strong><br/>
    <code>What do you know about Amsterdam?</code></p>

  <p>Tip: Try minimizing the number of iterations needed for a coherent answer!</p>

<h2>üìä Preliminary benchmark scores</h2>
<table>
  <tr><th>Benchmark</th><th>Score</th></tr>
  <tr><td>ARC-Easy</td><td>88.5</td></tr>
  <tr><td>ARC-Challenge</td><td>81.0</td></tr>
  <tr><td>MMLU</td><td>60.5</td></tr>
  <tr><td>HellaSwag</td><td>70.0</td></tr>
</table>

<p><em>Note:</em> Scores were calculated only on a 200 random subset of each dataset, detailed benchmark will folow.</p>

  <p>For a tweakable version with full inference parameters:<br/>
    <a href="https://huggingface.co/spaces/Ruurd/tini" target="_blank">Explore the model here</a>
  </p>

<h2>üìù How to Cite</h2>
<p>
If you use this model or demo in your work, please cite the blog post:
</p>

<pre><code>
@misc{kuiper2025lad,
  author       = {Ruurd Kuiper, Maarten van Smeden, Lars de Groot and Ayoub Bagheri},
  title        = {LAD: LlaMA Adapted Denoiser - A Diffusion-Based Language Model},
  howpublished = {\url{https://ruurdkuiper.github.io/tini-lad/}},
  year         = {2025},
  note         = {Blogpost, model and demo available online}
}
</code></pre>

<p>
We‚Äôll update this section once the official paper is available.
</p>

</body>
</html>
